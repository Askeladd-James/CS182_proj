import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, CosineAnnealingLR, CosineAnnealingWarmRestarts
from data_process import (load_data, create_time_aware_split, save_split_data, 
                         check_split_data_exists, load_existing_split_data, 
                         MovieLensDataset, data_path)
from MMOE import TwoStageMMoEModel
from torch.utils.data import DataLoader, Dataset
import logging
import pandas as pd
import numpy as np
import time
import math
from pathlib import Path

class OptimizedTemporalDataset(Dataset):
    """ä¼˜åŒ–çš„æ—¶åºè®­ç»ƒæ•°æ®é›† - ä½¿ç”¨ç»Ÿè®¡ç‰¹å¾è€Œä¸æ˜¯å®Œæ•´åºåˆ—"""
    def __init__(self, data, user_history_stats):
        self.data = data.reset_index(drop=True)
        self.user_history_stats = user_history_stats
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        user_id = row['user_emb_id']
        
        # è·å–ç”¨æˆ·å†å²ç»Ÿè®¡ç‰¹å¾
        if user_id in self.user_history_stats:
            history_features = torch.FloatTensor(self.user_history_stats[user_id])
        else:
            # é»˜è®¤å†å²ç‰¹å¾
            history_features = torch.FloatTensor([3.0, 1.0, 1.0, 3.0, 0.0])
        
        return (
            torch.LongTensor([row['user_emb_id']]),
            torch.LongTensor([row['movie_emb_id']]),
            torch.FloatTensor([row['rating']]),
            torch.LongTensor([row['daytime']]),
            torch.LongTensor([row['is_weekend']]),
            torch.LongTensor([row['year']]),
            history_features
        )

class FusionDataset(Dataset):
    """èåˆè®­ç»ƒæ•°æ®é›†"""
    def __init__(self, data, temporal_predictions, cf_predictions):
        self.data = data.reset_index(drop=True)
        self.temporal_predictions = temporal_predictions
        self.cf_predictions = cf_predictions
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        return (
            torch.LongTensor([row['user_emb_id']]),
            torch.LongTensor([row['movie_emb_id']]),
            torch.FloatTensor([row['rating']]),
            torch.LongTensor([row['daytime']]),
            torch.LongTensor([row['is_weekend']]),
            torch.LongTensor([row['year']]),
            torch.FloatTensor([self.temporal_predictions[idx]]),
            torch.FloatTensor([self.cf_predictions[idx]])
        )

def prepare_user_history_stats(train_data):
    """æ”¹è¿›çš„ç”¨æˆ·å†å²ç»Ÿè®¡ç‰¹å¾å‡†å¤‡"""
    user_stats = {}
    
    # æŒ‰ç”¨æˆ·åˆ†ç»„è®¡ç®—ç»Ÿè®¡ç‰¹å¾
    for user_id in train_data['user_emb_id'].unique():
        user_data = train_data[train_data['user_emb_id'] == user_id].copy()
        user_data = user_data.sort_values('timestamp')
        
        ratings = user_data['rating'].values
        
        if len(ratings) > 0:
            # è®¡ç®—æ›´ä¸°å¯Œçš„ç»Ÿè®¡ç‰¹å¾
            avg_rating = np.mean(ratings)
            std_rating = np.std(ratings) if len(ratings) > 1 else 0.1  # é¿å…0æ–¹å·®
            num_ratings = min(len(ratings), 200)  # é™åˆ¶æœ€å¤§å€¼é¿å…ç‰¹å¾è¿‡å¤§
            recent_rating = ratings[-1] if len(ratings) > 0 else avg_rating
            
            # æ”¹è¿›çš„è¯„åˆ†è¶‹åŠ¿è®¡ç®—
            if len(ratings) >= 3:
                # ä½¿ç”¨åŠ æƒå¹³å‡ï¼Œæ›´é‡è§†æœ€è¿‘çš„è¯„åˆ†
                weights = np.exp(np.linspace(-1, 0, len(ratings)))
                weighted_avg = np.average(ratings, weights=weights)
                trend = (weighted_avg - avg_rating) / max(std_rating, 0.1)
            else:
                trend = 0.0
            
            # å½’ä¸€åŒ–ç‰¹å¾
            user_stats[user_id] = [
                avg_rating,                    # å¹³å‡è¯„åˆ† [1-5]
                min(std_rating, 2.0),         # æ ‡å‡†å·®é™åˆ¶åœ¨åˆç†èŒƒå›´
                np.log1p(num_ratings) / 5.0,  # å¯¹æ•°å˜æ¢çš„è¯„åˆ†æ•°é‡
                recent_rating,                # æœ€è¿‘è¯„åˆ† [1-5]
                np.tanh(trend)                # tanhé™åˆ¶è¶‹åŠ¿åœ¨[-1,1]
            ]
        else:
            user_stats[user_id] = [3.0, 0.5, 0.1, 3.0, 0.0]
    
    return user_stats

def create_temporal_dataloader(data, user_history_stats, batch_size, shuffle=True):
    """åˆ›å»ºæ—¶åºæ•°æ®åŠ è½½å™¨"""
    dataset = OptimizedTemporalDataset(data, user_history_stats)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)

def create_standard_dataloader(data, batch_size, shuffle=True):
    """åˆ›å»ºæ ‡å‡†æ•°æ®åŠ è½½å™¨"""
    dataset = MovieLensDataset(
        data['user_emb_id'].values,
        data['movie_emb_id'].values,
        data['rating'].values,
        data['daytime'].values,
        data['is_weekend'].values,
        data['year'].values
    )
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)

def get_predictions_for_data(model, data, user_history_stats, device, stage):
    """ä¸ºæ•°æ®è·å–é¢„æµ‹ç»“æœ"""
    model.eval()
    model.set_training_stage(stage)
    
    predictions = []
    
    with torch.no_grad():
        for _, row in data.iterrows():
            user_id = torch.LongTensor([row['user_emb_id']]).to(device)
            item_id = torch.LongTensor([row['movie_emb_id']]).to(device)
            daytime = torch.LongTensor([row['daytime']]).to(device)
            weekend = torch.LongTensor([row['is_weekend']]).to(device)
            year = torch.LongTensor([row['year']]).to(device)
            
            if stage == 1:  # æ—¶åºé¢„æµ‹
                user_emb_id = row['user_emb_id']
                if user_emb_id in user_history_stats:
                    history_features = torch.FloatTensor(user_history_stats[user_emb_id]).unsqueeze(0).to(device)
                else:
                    history_features = torch.FloatTensor([3.0, 1.0, 1.0, 3.0, 0.0]).unsqueeze(0).to(device)
                
                pred = model(user_id, item_id, daytime, weekend, year, history_features)
            else:  # CFé¢„æµ‹
                pred = model(user_id, item_id, daytime, weekend, year)
            
            predictions.append(pred.item())
    
    return predictions

def get_predictions_for_data_batch(model, data, user_history_stats, device, stage, batch_size=1024):
    """æ‰¹é‡è·å–é¢„æµ‹ç»“æœ - ä¼˜åŒ–ç‰ˆæœ¬"""
    model.eval()
    model.set_training_stage(stage)
    
    predictions = []
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨è¿›è¡Œæ‰¹å¤„ç†
    if stage == 1:
        # æ—¶åºé¢„æµ‹éœ€è¦ç”¨æˆ·å†å²ç‰¹å¾
        dataset = OptimizedTemporalDataset(data, user_history_stats)
    else:
        # CFé¢„æµ‹ä½¿ç”¨æ ‡å‡†æ•°æ®é›†
        dataset = MovieLensDataset(
            data['user_emb_id'].values,
            data['movie_emb_id'].values,
            data['rating'].values,  # è¿™é‡Œçš„ratingä¸ä¼šè¢«ä½¿ç”¨ï¼Œåªæ˜¯ä¸ºäº†ä¿æŒæ¥å£ä¸€è‡´
            data['daytime'].values,
            data['is_weekend'].values,
            data['year'].values
        )
    
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    
    with torch.no_grad():
        for batch in dataloader:
            if stage == 1:
                # æ—¶åºé¢„æµ‹
                users, items, _, daytime, weekend, years, history_features = batch
                users = users.squeeze().to(device)
                items = items.squeeze().to(device)
                daytime = daytime.squeeze().to(device)
                weekend = weekend.squeeze().to(device)
                years = years.squeeze().to(device)
                history_features = history_features.to(device)
                
                batch_preds = model(users, items, daytime, weekend, years, history_features)
            else:
                # CFé¢„æµ‹
                users, items, _, daytime, weekend, years = batch
                users = users.squeeze().to(device)
                items = items.squeeze().to(device)
                daytime = daytime.squeeze().to(device)
                weekend = weekend.squeeze().to(device)
                years = years.squeeze().to(device)
                
                batch_preds = model(users, items, daytime, weekend, years)
            
            # å¤„ç†å•ä¸ªå€¼å’Œæ‰¹é‡å€¼çš„æƒ…å†µ
            if batch_preds.dim() == 0:
                predictions.append(batch_preds.item())
            else:
                predictions.extend(batch_preds.cpu().numpy().tolist())
    
    return predictions

def create_cached_fusion_dataloader(model, data, user_history_stats, batch_size, device, shuffle=True, cache_file=None):
    """åˆ›å»ºå¸¦ç¼“å­˜çš„èåˆæ•°æ®åŠ è½½å™¨"""
    
    if cache_file and Path(cache_file).exists():
        print(f"  ä»ç¼“å­˜åŠ è½½é¢„æµ‹ç»“æœ: {cache_file}")
        cache_data = torch.load(cache_file)
        temporal_predictions = cache_data['temporal_predictions']
        cf_predictions = cache_data['cf_predictions']
    else:
        print("  ç”Ÿæˆå¹¶ç¼“å­˜é¢„æµ‹ç»“æœ...")
        temporal_predictions = get_predictions_for_data_batch(model, data, user_history_stats, device, stage=1, batch_size=2048)
        cf_predictions = get_predictions_for_data_batch(model, data, user_history_stats, device, stage=2, batch_size=2048)
        
        if cache_file:
            cache_data = {
                'temporal_predictions': temporal_predictions,
                'cf_predictions': cf_predictions
            }
            torch.save(cache_data, cache_file)
            print(f"  é¢„æµ‹ç»“æœå·²ç¼“å­˜è‡³: {cache_file}")
    
    dataset = FusionDataset(data, temporal_predictions, cf_predictions)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)

def train_stage(model, train_loader, val_loader, criterion, optimizer, device, 
                         num_epochs, stage_name, patience=5, scheduler=None, stage_num=1):
    """ä¼˜åŒ–çš„è®­ç»ƒé˜¶æ®µå‡½æ•°"""
    best_loss = float('inf')
    best_model_state = None
    patience_counter = 0
    
    # è®°å½•è®­ç»ƒå†å²
    train_losses = []
    val_losses = []
    learning_rates = []
    
    # æ ¹æ®ä¸åŒé˜¶æ®µè°ƒæ•´æ¢¯åº¦è£å‰ª
    max_grad_norms = {1: 0.5, 2: 1.0, 3: 1.5}  # æ—¶åºé˜¶æ®µç”¨æ›´å°çš„æ¢¯åº¦è£å‰ª
    max_grad_norm = max_grad_norms.get(stage_num, 1.0)
    
    print(f"å¼€å§‹ {stage_name} è®­ç»ƒï¼Œæœ€å¤§æ¢¯åº¦è£å‰ª: {max_grad_norm}")
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0
        reg_loss_total = 0
        num_batches = 0
        
        for batch_idx, batch in enumerate(train_loader):
            optimizer.zero_grad()
            
            try:
                if model.training_stage == 1:
                    # æ—¶åºè®­ç»ƒ - éœ€è¦7ä¸ªå…ƒç´ ï¼ˆåŒ…æ‹¬history_featuresï¼‰
                    users, items, ratings, daytime, weekend, years, history_features = batch
                    users = users.squeeze().to(device)
                    items = items.squeeze().to(device)
                    ratings = ratings.squeeze().to(device)
                    daytime = daytime.squeeze().to(device)
                    weekend = weekend.squeeze().to(device)
                    years = years.squeeze().to(device)
                    history_features = history_features.to(device)
                    
                    predictions = model(users, items, daytime, weekend, years, history_features)
                    targets = ratings
                    
                elif model.training_stage == 2:
                    # CFè®­ç»ƒ
                    users, items, ratings, daytime, weekend, years, *extra_features = batch
                    users = users.squeeze().to(device)
                    items = items.squeeze().to(device)
                    ratings = ratings.squeeze().to(device)
                    daytime = daytime.squeeze().to(device)
                    weekend = weekend.squeeze().to(device)
                    years = years.squeeze().to(device)
                    
                    predictions = model(users, items, daytime, weekend, years)
                    targets = ratings
                    
                elif model.training_stage == 3:
                    # MMoEè®­ç»ƒ
                    users, items, ratings, daytime, weekend, years, *extra_features = batch
                    users = users.squeeze().to(device)
                    items = items.squeeze().to(device)
                    ratings = ratings.squeeze().to(device)
                    daytime = daytime.squeeze().to(device)
                    weekend = weekend.squeeze().to(device)
                    years = years.squeeze().to(device)
                    
                    if len(extra_features) == 2:
                        # FusionDataset
                        temporal_preds, cf_preds = extra_features
                        temporal_preds = temporal_preds.squeeze().to(device)
                        cf_preds = cf_preds.squeeze().to(device)
                        predictions = model(users, items, daytime, weekend, years, temporal_preds, cf_preds)
                    else:
                        predictions = model(users, items, daytime, weekend, years)
                    
                    targets = ratings
                
                # è®¡ç®—æŸå¤±
                mse_loss = criterion(predictions, targets)
                reg_loss = model.get_regularization_loss()
                total_loss_batch = mse_loss + reg_loss
                
                # åå‘ä¼ æ’­
                total_loss_batch.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)
                
                optimizer.step()
                
                total_loss += mse_loss.item()
                reg_loss_total += reg_loss.item()
                num_batches += 1
                
                # æ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦ï¼ˆä»…åœ¨ç¬¬ä¸€ä¸ªepochï¼‰
                if epoch == 0 and batch_idx % 100 == 0:
                    print(f"  Batch {batch_idx}/{len(train_loader)}, Loss: {mse_loss.item():.4f}")
                    
            except Exception as e:
                print(f"è®­ç»ƒæ‰¹æ¬¡é”™è¯¯: {e}")
                continue
        
        if num_batches == 0:
            print(f"è­¦å‘Š: {stage_name} Epoch {epoch+1} æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ‰¹æ¬¡")
            continue
            
        avg_train_loss = total_loss / num_batches
        avg_reg_loss = reg_loss_total / num_batches
        train_losses.append(avg_train_loss)
        
        # è®°å½•å½“å‰å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']
        learning_rates.append(current_lr)
        
        # éªŒè¯é˜¶æ®µ
        if val_loader is not None:
            val_loss = evaluate_stage(model, val_loader, criterion, device)
            val_losses.append(val_loss)
            
            print(f"{stage_name} Epoch {epoch+1}/{num_epochs}, "
                  f"Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, "
                  f"Reg Loss: {avg_reg_loss:.4f}, LR: {current_lr:.6f}")
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            old_lr = current_lr
            if scheduler is not None:
                if isinstance(scheduler, ReduceLROnPlateau):
                    scheduler.step(val_loss)
                elif isinstance(scheduler, CosineAnnealingWarmRestarts):
                    scheduler.step()
                else:
                    scheduler.step()
                
                # æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦æ”¹å˜
                new_lr = optimizer.param_groups[0]['lr']
                if abs(new_lr - old_lr) > 1e-8:
                    print(f"  â†’ å­¦ä¹ ç‡è°ƒæ•´: {old_lr:.6f} â†’ {new_lr:.6f}")
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_loss:
                best_loss = val_loss
                best_model_state = model.state_dict().copy()
                patience_counter = 0
                print(f"  â†’ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {val_loss:.4f}")
            else:
                patience_counter += 1
                print(f"  â†’ éªŒè¯æŸå¤±æœªæ”¹å–„ ({patience_counter}/{patience})")
                
            # æ—©åœ
            if patience_counter >= patience:
                print(f"  â†’ æ—©åœï¼šéªŒè¯æŸå¤±è¿ç»­ {patience} è½®æœªæ”¹å–„")
                break
        else:
            print(f"{stage_name} Epoch {epoch+1}/{num_epochs}, "
                  f"Train Loss: {avg_train_loss:.4f}, Reg Loss: {avg_reg_loss:.4f}, LR: {current_lr:.6f}")
                    
            if avg_train_loss < best_loss:
                best_loss = avg_train_loss
                best_model_state = model.state_dict().copy()
    
    # æ¢å¤æœ€ä½³æ¨¡å‹çŠ¶æ€
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"  â†’ å·²æ¢å¤åˆ°æœ€ä½³æ¨¡å‹çŠ¶æ€ (æœ€ä½³æŸå¤±: {best_loss:.4f})")
    
    # è¿”å›è®­ç»ƒå†å²
    training_history = {
        'train_losses': train_losses,
        'val_losses': val_losses if val_loader is not None else [],
        'learning_rates': learning_rates,
        'best_loss': best_loss,
        'total_epochs': epoch + 1
    }
    
    return best_loss, training_history

def evaluate_stage(model, val_loader, criterion, device):
    """è¯„ä¼°é˜¶æ®µæ€§èƒ½ - ä¿®å¤æ‰€æœ‰é˜¶æ®µçš„batchè§£åŒ…é—®é¢˜"""
    model.eval()
    total_loss = 0
    num_batches = 0
    
    with torch.no_grad():
        for batch in val_loader:
            if model.training_stage == 1:
                # æ—¶åºéªŒè¯ - 7ä¸ªå…ƒç´ 
                users, items, ratings, daytime, weekend, years, history_features = batch
                users = users.squeeze().to(device)
                items = items.squeeze().to(device)
                ratings = ratings.squeeze().to(device)
                daytime = daytime.squeeze().to(device)
                weekend = weekend.squeeze().to(device)
                years = years.squeeze().to(device)
                history_features = history_features.to(device)
                
                predictions = model(users, items, daytime, weekend, years, history_features)
                targets = ratings
                
            elif model.training_stage == 2:
                # CFéªŒè¯ - ä½¿ç”¨*extra_featureså¤„ç†å¯èƒ½çš„å˜é•¿å‚æ•°
                users, items, ratings, daytime, weekend, years, *extra_features = batch
                users = users.squeeze().to(device)
                items = items.squeeze().to(device)
                ratings = ratings.squeeze().to(device)
                daytime = daytime.squeeze().to(device)
                weekend = weekend.squeeze().to(device)
                years = years.squeeze().to(device)
                
                predictions = model(users, items, daytime, weekend, years)
                targets = ratings
                
            elif model.training_stage == 3:
                # MMoEéªŒè¯ - ä½¿ç”¨*extra_featureså¤„ç†å¯å˜æ•°é‡çš„å…ƒç´ 
                users, items, ratings, daytime, weekend, years, *extra_features = batch
                users = users.squeeze().to(device)
                items = items.squeeze().to(device)
                ratings = ratings.squeeze().to(device)
                daytime = daytime.squeeze().to(device)
                weekend = weekend.squeeze().to(device)
                years = years.squeeze().to(device)
                
                if len(extra_features) == 2:
                    # FusionDataset - æœ‰temporal_predså’Œcf_preds
                    temporal_preds, cf_preds = extra_features
                    temporal_preds = temporal_preds.squeeze().to(device)
                    cf_preds = cf_preds.squeeze().to(device)
                    predictions = model(users, items, daytime, weekend, years, temporal_preds, cf_preds)
                else:
                    # æ ‡å‡†æ•°æ®é›† - æ²¡æœ‰é¢å¤–ç‰¹å¾
                    predictions = model(users, items, daytime, weekend, years)
                
                targets = ratings
            
            loss = criterion(predictions, targets)
            total_loss += loss.item()
            num_batches += 1
    
    return total_loss / num_batches

def train_mmoe(model, train_data, val_data, device, batch_size=256, 
               num_epochs_per_stage=[40, 50, 40], learning_rates=[0.001, 0.002, 0.0005]):
    """ä¿®å¤çš„MMoEè®­ç»ƒå‡½æ•° - æ­£ç¡®è®°å½•è®­ç»ƒæ—¶é—´"""
    
    print("å‡†å¤‡ç”¨æˆ·å†å²ç»Ÿè®¡ç‰¹å¾...")
    user_history_stats = prepare_user_history_stats(train_data)
    val_user_history_stats = prepare_user_history_stats(val_data)
    
    criterion = nn.MSELoss()
    
    # ğŸ”§ ä¿®å¤ï¼šåˆ›å»ºç»Ÿä¸€çš„è®­ç»ƒå†å²è®°å½•
    unified_training_history = {
        'train_losses': [],
        'val_losses': [],
        'train_rmse': [],
        'val_rmse': [],
        'learning_rates': [],
        'epoch_times': [],
        'best_epoch': 0,
        'total_epochs': 0,
        'total_training_time': 0.0,
        'stage_info': []  # è®°å½•æ¯ä¸ªé˜¶æ®µçš„ä¿¡æ¯
    }
    
    # ğŸ”§ è®°å½•æ€»çš„å¼€å§‹æ—¶é—´
    total_start_time = time.time()
    
    # é˜¶æ®µ1ï¼šæ—¶åºå»ºæ¨¡
    print("=" * 50)
    print("é˜¶æ®µ1ï¼šæ—¶åºå»ºæ¨¡")
    print("=" * 50)
    
    stage1_start = time.time()
    model.set_training_stage(1)
    temporal_loader = create_temporal_dataloader(train_data, user_history_stats, batch_size)
    temporal_val_loader = create_temporal_dataloader(val_data, val_user_history_stats, batch_size, shuffle=False)
    
    optimizer1 = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()), 
        lr=learning_rates[0], 
        weight_decay=1e-5,
        betas=(0.9, 0.999)
    )
    
    scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer1, mode='min', factor=0.7, patience=5, min_lr=1e-7
    )
    
    best_temporal_loss, temporal_history = train_stage(
        model, temporal_loader, temporal_val_loader, criterion, optimizer1, 
        device, num_epochs_per_stage[0], "Temporal", patience=5, scheduler=scheduler1, stage_num=1
    )
    
    stage1_time = time.time() - stage1_start
    
    # ğŸ”§ åˆå¹¶é˜¶æ®µ1çš„å†å²åˆ°ç»Ÿä¸€å†å²
    unified_training_history['train_losses'].extend(temporal_history['train_losses'])
    unified_training_history['val_losses'].extend(temporal_history['val_losses'])
    if 'learning_rates' in temporal_history:
        unified_training_history['learning_rates'].extend(temporal_history['learning_rates'])
    if 'epoch_times' in temporal_history:
        unified_training_history['epoch_times'].extend(temporal_history['epoch_times'])
    
    # è®¡ç®—RMSE
    temporal_rmse = [math.sqrt(loss) for loss in temporal_history['train_losses']]
    temporal_val_rmse = [math.sqrt(loss) for loss in temporal_history['val_losses']]
    unified_training_history['train_rmse'].extend(temporal_rmse)
    unified_training_history['val_rmse'].extend(temporal_val_rmse)
    
    # è®°å½•é˜¶æ®µä¿¡æ¯
    unified_training_history['stage_info'].append({
        'stage_name': 'Temporal',
        'start_epoch': 0,
        'end_epoch': temporal_history['total_epochs'] - 1,
        'epochs': temporal_history['total_epochs'],
        'best_loss': best_temporal_loss,
        'training_time': stage1_time
    })
    
    print(f"é˜¶æ®µ1å®Œæˆ - è®­ç»ƒæ—¶é—´: {stage1_time:.2f}s, æœ€ä½³æŸå¤±: {best_temporal_loss:.4f}")
    
    # é˜¶æ®µ2ï¼šCFå»ºæ¨¡
    print("=" * 50)
    print("é˜¶æ®µ2ï¼šååŒè¿‡æ»¤å»ºæ¨¡")
    print("=" * 50)
    
    stage2_start = time.time()
    model.set_training_stage(2)
    cf_loader = create_standard_dataloader(train_data, batch_size)
    val_loader = create_standard_dataloader(val_data, batch_size, shuffle=False)
    
    optimizer2 = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()), 
        lr=learning_rates[1], 
        weight_decay=1e-5,
        betas=(0.9, 0.999)
    )
    
    scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer2, mode='min', factor=0.6, patience=5, min_lr=1e-7
    )
    
    best_cf_loss, cf_history = train_stage(
        model, cf_loader, val_loader, criterion, optimizer2, 
        device, num_epochs_per_stage[1], "CF", patience=5, scheduler=scheduler2, stage_num=2
    )
    
    stage2_time = time.time() - stage2_start
    
    # ğŸ”§ åˆå¹¶é˜¶æ®µ2çš„å†å²
    current_epoch_offset = len(unified_training_history['train_losses'])
    unified_training_history['train_losses'].extend(cf_history['train_losses'])
    unified_training_history['val_losses'].extend(cf_history['val_losses'])
    if 'learning_rates' in cf_history:
        unified_training_history['learning_rates'].extend(cf_history['learning_rates'])
    if 'epoch_times' in cf_history:
        unified_training_history['epoch_times'].extend(cf_history['epoch_times'])
    
    # è®¡ç®—RMSE
    cf_rmse = [math.sqrt(loss) for loss in cf_history['train_losses']]
    cf_val_rmse = [math.sqrt(loss) for loss in cf_history['val_losses']]
    unified_training_history['train_rmse'].extend(cf_rmse)
    unified_training_history['val_rmse'].extend(cf_val_rmse)
    
    # è®°å½•é˜¶æ®µä¿¡æ¯
    unified_training_history['stage_info'].append({
        'stage_name': 'CF',
        'start_epoch': current_epoch_offset,
        'end_epoch': current_epoch_offset + cf_history['total_epochs'] - 1,
        'epochs': cf_history['total_epochs'],
        'best_loss': best_cf_loss,
        'training_time': stage2_time
    })
    
    print(f"é˜¶æ®µ2å®Œæˆ - è®­ç»ƒæ—¶é—´: {stage2_time:.2f}s, æœ€ä½³æŸå¤±: {best_cf_loss:.4f}")
    
    # é˜¶æ®µ3ï¼šMMoEèåˆ
    print("=" * 50)
    print("é˜¶æ®µ3ï¼šMMoEèåˆ")
    print("=" * 50)
    
    stage3_start = time.time()
    model.set_training_stage(3)
    
    # æ¸…é™¤æ—§ç¼“å­˜
    train_cache_file = data_path + 'cache_train_predictions.pt'
    val_cache_file = data_path + 'cache_val_predictions.pt'
    
    for cache_file in [train_cache_file, val_cache_file]:
        if Path(cache_file).exists():
            Path(cache_file).unlink()
            print(f"  æ¸…é™¤ç¼“å­˜: {cache_file}")
    
    print("å‡†å¤‡èåˆè®­ç»ƒæ•°æ®...")
    fusion_loader = create_cached_fusion_dataloader(
        model, train_data, user_history_stats, batch_size, device, 
        cache_file=train_cache_file
    )
    
    print("å‡†å¤‡èåˆéªŒè¯æ•°æ®...")
    fusion_val_loader = create_cached_fusion_dataloader(
        model, val_data, val_user_history_stats, batch_size, device, 
        shuffle=False, cache_file=val_cache_file
    )
    
    optimizer3 = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()), 
        lr=learning_rates[2], 
        weight_decay=1e-6,
        betas=(0.9, 0.999)
    )
    
    scheduler3 = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer3, mode='min', factor=0.8, patience=5, min_lr=1e-8
    )
    
    best_mmoe_loss, mmoe_history = train_stage(
        model, fusion_loader, fusion_val_loader, criterion, optimizer3, 
        device, num_epochs_per_stage[2], "MMoE", patience=5, scheduler=scheduler3, stage_num=3
    )
    
    stage3_time = time.time() - stage3_start
    
    # ğŸ”§ åˆå¹¶é˜¶æ®µ3çš„å†å²
    current_epoch_offset = len(unified_training_history['train_losses'])
    unified_training_history['train_losses'].extend(mmoe_history['train_losses'])
    unified_training_history['val_losses'].extend(mmoe_history['val_losses'])
    if 'learning_rates' in mmoe_history:
        unified_training_history['learning_rates'].extend(mmoe_history['learning_rates'])
    if 'epoch_times' in mmoe_history:
        unified_training_history['epoch_times'].extend(mmoe_history['epoch_times'])
    
    # è®¡ç®—RMSE
    mmoe_rmse = [math.sqrt(loss) for loss in mmoe_history['train_losses']]
    mmoe_val_rmse = [math.sqrt(loss) for loss in mmoe_history['val_losses']]
    unified_training_history['train_rmse'].extend(mmoe_rmse)
    unified_training_history['val_rmse'].extend(mmoe_val_rmse)
    
    # è®°å½•é˜¶æ®µä¿¡æ¯
    unified_training_history['stage_info'].append({
        'stage_name': 'MMoE',
        'start_epoch': current_epoch_offset,
        'end_epoch': current_epoch_offset + mmoe_history['total_epochs'] - 1,
        'epochs': mmoe_history['total_epochs'],
        'best_loss': best_mmoe_loss,
        'training_time': stage3_time
    })
    
    print(f"é˜¶æ®µ3å®Œæˆ - è®­ç»ƒæ—¶é—´: {stage3_time:.2f}s, æœ€ä½³æŸå¤±: {best_mmoe_loss:.4f}")
    
    # ğŸ”§ è®¡ç®—æ€»çš„è®­ç»ƒæ—¶é—´å’Œç»Ÿè®¡ä¿¡æ¯
    total_training_time = time.time() - total_start_time
    unified_training_history['total_training_time'] = total_training_time
    unified_training_history['total_epochs'] = len(unified_training_history['train_losses'])
    
    # æ‰¾åˆ°æœ€ä½³éªŒè¯æŸå¤±çš„epoch
    if unified_training_history['val_losses']:
        best_val_idx = np.argmin(unified_training_history['val_losses'])
        unified_training_history['best_epoch'] = best_val_idx
    
    # è¯¦ç»†çš„æ€§èƒ½åˆ†æ
    print("=" * 60)
    print("ğŸ¯ è®­ç»ƒå®Œæˆ! è¯¦ç»†ç»“æœåˆ†æ:")
    print("=" * 60)
    
    print(f"ğŸ“Š å„é˜¶æ®µæ€§èƒ½:")
    print(f"  æ—¶åºå»ºæ¨¡æŸå¤±: {best_temporal_loss:.6f} ({temporal_history['total_epochs']} epochs, {stage1_time:.1f}s)")
    print(f"  CFå»ºæ¨¡æŸå¤±:   {best_cf_loss:.6f} ({cf_history['total_epochs']} epochs, {stage2_time:.1f}s)")
    print(f"  MMoEèåˆæŸå¤±: {best_mmoe_loss:.6f} ({mmoe_history['total_epochs']} epochs, {stage3_time:.1f}s)")
    print(f"  æ€»è®­ç»ƒè½®æ•°:   {unified_training_history['total_epochs']}")
    print(f"  æ€»è®­ç»ƒæ—¶é—´:   {total_training_time:.2f}s")
    
    # æ”¹è¿›æå‡åˆ†æ
    single_model_best = min(best_temporal_loss, best_cf_loss)
    if best_mmoe_loss < single_model_best:
        improvement = ((single_model_best - best_mmoe_loss) / single_model_best * 100)
        print(f"âœ… MMoEç›¸å¯¹æœ€ä½³å•æ¨¡å‹æå‡: {improvement:.2f}%")
    else:
        degradation = ((best_mmoe_loss - single_model_best) / single_model_best * 100)
        print(f"âš ï¸  MMoEç›¸å¯¹æœ€ä½³å•æ¨¡å‹å˜åŒ–: +{degradation:.2f}%")
    
    print("=" * 60)
    
    # ğŸ”§ ä¿®å¤ï¼šè¿”å›ç»Ÿä¸€çš„è®­ç»ƒå†å²ï¼Œè€Œä¸æ˜¯åˆ†é˜¶æ®µçš„
    return model, unified_training_history

def main():
    """ä¸»å‡½æ•° - ä¿æŒåŸæœ‰ç»“æ„ï¼Œé€‚é…æ”¹è¿›æ¨¡å‹"""
    logging.basicConfig(level=logging.INFO)
    
    # é…ç½®å‚æ•°è°ƒæ•´ä»¥é€‚é…æ”¹è¿›æ¨¡å‹
    config = {
        'DEVICE': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),
        'K_FACTORS': 100,  # ä¿æŒä¸UMTimeModelä¸€è‡´
        'TIME_FACTORS': 40,
        'BATCH_SIZE': 256,
        'NUM_EPOCHS_PER_STAGE': [30, 30, 30],  # CFé˜¶æ®µè½®æ•°é€‚é…æ”¹è¿›å±‚
        'LEARNING_RATES': [0.0005, 0.001, 0.001],
        'REG_STRENGTH': 0.0005,
        'NUM_EXPERTS': 4
    }
    
    print(f'ä½¿ç”¨è®¾å¤‡: {config["DEVICE"]}')
    print(f'MMOEæ¨¡å‹é…ç½®: {config}')
    
    # æ•°æ®åŠ è½½ï¼ˆä¿æŒä¸å˜ï¼‰
    split_path = data_path + 'split_data'
    
    if check_split_data_exists(split_path):
        print("åŠ è½½ç°æœ‰æ•°æ®åˆ†å‰²...")
        train_data, val_data, test_data = load_existing_split_data(split_path)
    else:
        print("åˆ›å»ºæ–°çš„æ•°æ®åˆ†å‰²...")
        ratings, users, movies = load_data(
            data_path + 'ratings.csv',
            data_path + 'users.csv', 
            data_path + 'movies.csv'
        )
        train_data, val_data, test_data = create_time_aware_split(ratings, random_state=42)
        save_split_data(train_data, val_data, test_data, split_path)
    
    print(f'æ•°æ®åˆ†å‰² - è®­ç»ƒ: {len(train_data)}, éªŒè¯: {len(val_data)}, æµ‹è¯•: {len(test_data)}')
    
    max_userid = train_data['user_emb_id'].max()
    max_movieid = train_data['movie_emb_id'].max()
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆç°åœ¨ä½¿ç”¨æ”¹è¿›çš„MMOEï¼Œä½†ä¿æŒåŸæœ‰æ¥å£ï¼‰
    model = TwoStageMMoEModel(
        max_userid + 1, max_movieid + 1,
        config['K_FACTORS'], config['TIME_FACTORS'],
        config['REG_STRENGTH'], config['NUM_EXPERTS']
    ).to(config['DEVICE'])
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"MMOEæ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
    
    # è®­ç»ƒæ¨¡å‹ï¼ˆä¿æŒåŸæœ‰æ¥å£ï¼‰
    model, training_history = train_mmoe(
        model, train_data, val_data, config['DEVICE'],
        batch_size=config['BATCH_SIZE'],
        num_epochs_per_stage=config['NUM_EPOCHS_PER_STAGE'],
        learning_rates=config['LEARNING_RATES']
    )
    
    # ä¿å­˜æ¨¡å‹ï¼ˆä¿æŒåŸæœ‰æ ¼å¼ï¼Œä½†åŒ…å«æ”¹è¿›ä¿¡æ¯ï¼‰
    checkpoint = {
        'max_userid': max_userid,
        'max_movieid': max_movieid,
        'k_factors': config['K_FACTORS'],
        'time_factors': config['TIME_FACTORS'],
        'reg_strength': config['REG_STRENGTH'],
        'num_experts': config['NUM_EXPERTS'],
        'best_model_state': model.state_dict(),
        'model_type': model.name,
        'data_split_path': split_path,
        'training_history': training_history,
        'config': config,
        'has_scheduler': True
    }
    
    model_path = data_path + f'model/model_checkpoint_{model.name}.pt'
    torch.save(checkpoint, model_path, _use_new_zipfile_serialization=False)
    print(f'æ¨¡å‹ä¿å­˜è‡³: {model_path}')
    
    return model, test_data, training_history

if __name__ == "__main__":
    main()